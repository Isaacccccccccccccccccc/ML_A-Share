{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399be6ed-c08d-4438-b601-d637f093357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code is designed to run on the JoinQuant research platform and relies on\n",
    "JoinQuant-specific APIs and data services. As a result, it cannot be executed directly\n",
    "in a standard local Python environment.\n",
    "\n",
    "Therefore, although this script may not run locally without errors, it is fully\n",
    "functional and executable when deployed within the JoinQuant platform with the\n",
    "appropriate permissions and data access rights.\n",
    "\"\"\"\n",
    "\n",
    "from jqdata import *\n",
    "from jqfactor import get_factor_values\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import pickle\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class MultiModelResearch:\n",
    "    \"\"\"Multi-model ML research framework\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Factor list\n",
    "        self.jqfactors_list = [\n",
    "            'asset_impairment_loss_ttm', 'cash_flow_to_price_ratio', 'market_cap', \n",
    "            'interest_free_current_liability', 'EBITDA', 'financial_assets', \n",
    "            'gross_profit_ttm', 'net_working_capital', 'non_recurring_gain_loss', 'EBIT',\n",
    "            'sales_to_price_ratio', 'AR', 'ARBR', 'ATR6', 'DAVOL10', 'MAWVAD', 'TVMA6', \n",
    "            'PSY', 'VOL10', 'VDIFF', 'VEMA26', 'VMACD', 'VOL120', 'VOSC', 'VR', 'WVAD', \n",
    "            'arron_down_25', 'arron_up_25', 'BBIC', 'MASS', 'Rank1M', 'single_day_VPT', \n",
    "            'single_day_VPT_12', 'single_day_VPT_6', 'Volume1M',\n",
    "            'capital_reserve_fund_per_share', 'net_asset_per_share', \n",
    "            'net_operate_cash_flow_per_share', 'operating_profit_per_share', \n",
    "            'total_operating_revenue_per_share', 'surplus_reserve_fund_per_share',\n",
    "            'ACCA', 'account_receivable_turnover_days', 'account_receivable_turnover_rate', \n",
    "            'adjusted_profit_to_total_profit', 'super_quick_ratio', 'MLEV', \n",
    "            'debt_to_equity_ratio', 'debt_to_tangible_equity_ratio', \n",
    "            'equity_to_fixed_asset_ratio', 'fixed_asset_ratio', 'intangible_asset_ratio', \n",
    "            'invest_income_associates_to_total_profit', 'long_debt_to_asset_ratio',\n",
    "            'long_debt_to_working_capital_ratio', 'net_operate_cash_flow_to_total_liability', \n",
    "            'net_operating_cash_flow_coverage', 'non_current_asset_ratio', \n",
    "            'operating_profit_to_total_profit', 'roa_ttm', 'roe_ttm', 'Kurtosis120', \n",
    "            'Kurtosis20', 'Kurtosis60', 'sharpe_ratio_20', 'sharpe_ratio_60', \n",
    "            'Skewness120', 'Skewness20', 'Skewness60', 'Variance120', 'Variance20', \n",
    "            'liquidity', 'beta', 'book_to_price_ratio', 'cash_earnings_to_price_ratio', \n",
    "            'cube_of_size', 'earnings_to_price_ratio', 'earnings_yield', 'growth', \n",
    "            'momentum', 'natural_log_of_market_cap', 'boll_down', 'MFI14', 'MAC10', \n",
    "            'fifty_two_week_close_rank', 'price_no_fq'\n",
    "        ]\n",
    "        \n",
    "        # Model registry\n",
    "        self.models_config = {\n",
    "            'lgb': {'name': 'LightGBM', 'model': None},\n",
    "            'xgb': {'name': 'XGBoost', 'model': None},\n",
    "            'svr': {'name': 'SVR', 'model': None},\n",
    "            'rf': {'name': 'RandomForest', 'model': None},\n",
    "            'lr': {'name': 'LinearRegression', 'model': None},\n",
    "            'ensemble': {'name': 'Ensemble', 'model': None}\n",
    "        }\n",
    "        \n",
    "    def get_period_date(self, period, start_date, end_date):\n",
    "        \"\"\"Get date list within a time window.\"\"\"\n",
    "        stock_data = get_price('000001.XSHE', start_date, end_date, 'daily', fields=['close'])\n",
    "        stock_data['date'] = stock_data.index\n",
    "        period_stock_data = stock_data.resample(period).last()\n",
    "        period_stock_data = period_stock_data.set_index('date').dropna()\n",
    "        date = period_stock_data.index\n",
    "        pydate_array = date.to_pydatetime()\n",
    "        date_only_array = np.vectorize(lambda s: s.strftime('%Y-%m-%d'))(pydate_array)\n",
    "        date_only_series = pd.Series(date_only_array)\n",
    "        start_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        start_date_dt = start_date_dt - datetime.timedelta(days=1)\n",
    "        start_date_str = start_date_dt.strftime(\"%Y-%m-%d\")\n",
    "        date_list = date_only_series.values.tolist()\n",
    "        date_list.insert(0, start_date_str)\n",
    "        return date_list\n",
    "\n",
    "    def filter_new_stocks(self, stocks, begin_date, n=90):\n",
    "        \"\"\"Filter stocks with listing age < n days before begin_date.\"\"\"\n",
    "        stockList = []\n",
    "        beginDate = datetime.datetime.strptime(begin_date, \"%Y-%m-%d\")\n",
    "        for stock in stocks:\n",
    "            start_date = get_security_info(stock).start_date\n",
    "            if start_date < (beginDate - datetime.timedelta(days=n)).date():\n",
    "                stockList.append(stock)\n",
    "        return stockList\n",
    "\n",
    "    def get_stock_pool(self, stock_pool, begin_date):\n",
    "        \"\"\"Get universe by pool name on a given date.\"\"\"\n",
    "        if stock_pool == 'HS300':\n",
    "            stockList = get_index_stocks('000300.XSHG', begin_date)\n",
    "        elif stock_pool == 'ZZ500':\n",
    "            stockList = get_index_stocks('399905.XSHE', begin_date)\n",
    "        elif stock_pool == 'ZZ800':\n",
    "            stockList = get_index_stocks('399906.XSHE', begin_date)\n",
    "        elif stock_pool == 'CYBZ':\n",
    "            stockList = get_index_stocks('399006.XSHE', begin_date)\n",
    "        elif stock_pool == 'ZXBZ':\n",
    "            stockList = get_index_stocks('399101.XSHE', begin_date)\n",
    "        elif stock_pool == 'A':\n",
    "            stockList = get_index_stocks('000002.XSHG', begin_date) + get_index_stocks('399107.XSHE', begin_date)\n",
    "            stockList = [stock for stock in stockList if not stock.startswith(('68', '4', '8'))]\n",
    "        elif stock_pool == 'AA':\n",
    "            stockList = get_index_stocks('000985.XSHG', begin_date)\n",
    "            stockList = [stock for stock in stockList if not stock.startswith(('3', '68', '4', '8'))]\n",
    "        \n",
    "        st_data = get_extras('is_st', stockList, count=1, end_date=begin_date)\n",
    "        stockList = [stock for stock in stockList if not st_data[stock][0]]\n",
    "        stockList = self.filter_new_stocks(stockList, begin_date)\n",
    "        return stockList\n",
    "\n",
    "    def get_factor_data(self, securities_list, date):\n",
    "        \"\"\"Fetch factor cross-section for a date.\"\"\"\n",
    "        factor_data = get_factor_values(securities=securities_list, factors=self.jqfactors_list, count=1, end_date=date)\n",
    "        df_jq_factor = pd.DataFrame(index=securities_list)\n",
    "        for factor in factor_data.keys():\n",
    "            df_jq_factor[factor] = factor_data[factor].iloc[0, :]\n",
    "        return df_jq_factor\n",
    "\n",
    "    def generate_training_data(self):\n",
    "        \"\"\"Build training set.\"\"\"\n",
    "        print(\"Generating multi-model training data...\")\n",
    "        \n",
    "        period = 'M'\n",
    "        start_date = '2019-01-01'\n",
    "        end_date = '2023-01-01'\n",
    "        stock_pool = 'ZXBZ'\n",
    "        \n",
    "        dateList = self.get_period_date(period, start_date, end_date)\n",
    "        print(f\"Collected {len(dateList)} time points\")\n",
    "        \n",
    "        DF = pd.DataFrame()\n",
    "        \n",
    "        for i in tqdm(range(len(dateList)-1)):\n",
    "            date = dateList[i]\n",
    "            next_date = dateList[i+1]\n",
    "            \n",
    "            try:\n",
    "                stockList = self.get_stock_pool(stock_pool, date)\n",
    "                if len(stockList) == 0:\n",
    "                    continue\n",
    "                \n",
    "                factor_data = self.get_factor_data(stockList, date)\n",
    "                \n",
    "                data_close = get_price(stockList, date, next_date, '1d', fields=['close'])['close']\n",
    "                factor_data['pchg'] = data_close.iloc[-1] / data_close.iloc[0] - 1\n",
    "                \n",
    "                factor_data = factor_data.dropna()\n",
    "                \n",
    "                median_pchg = factor_data['pchg'].median()\n",
    "                factor_data['label'] = np.where(factor_data['pchg'] >= median_pchg, 1, 0)\n",
    "                factor_data = factor_data.drop(columns=['pchg'])\n",
    "                \n",
    "                DF = pd.concat([DF, factor_data], ignore_index=True)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error on date {date}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        selected_features = self.feature_selection(DF)\n",
    "        print(f\"Selected features: {len(selected_features)}\")\n",
    "        \n",
    "        final_data = DF[selected_features + ['label']]\n",
    "        \n",
    "        final_data.to_csv('multi_model_train_data.csv', index=False)\n",
    "        print(f\"Saved training data: multi_model_train_data.csv (shape: {final_data.shape})\")\n",
    "        \n",
    "        with open('multi_model_selected_features.pkl', 'wb') as f:\n",
    "            pickle.dump(selected_features, f)\n",
    "        print(\"Saved feature list: multi_model_selected_features.pkl\")\n",
    "        \n",
    "        return final_data, selected_features\n",
    "\n",
    "    def feature_selection(self, df):\n",
    "        \"\"\"Correlation-based feature selection.\"\"\"\n",
    "        print(\"Running feature selection...\")\n",
    "        \n",
    "        missing_counts = df[self.jqfactors_list].isnull().sum().to_dict()\n",
    "        corr_matrix = df[self.jqfactors_list].corr()\n",
    "        \n",
    "        graph = {}\n",
    "        threshold = 0.6\n",
    "        \n",
    "        n = len(self.jqfactors_list)\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                col1, col2 = self.jqfactors_list[i], self.jqfactors_list[j]\n",
    "                corr_value = corr_matrix.iloc[i, j]\n",
    "                \n",
    "                if not pd.isna(corr_value) and abs(corr_value) > threshold:\n",
    "                    if col1 not in graph:\n",
    "                        graph[col1] = []\n",
    "                    graph[col1].append(col2)\n",
    "                    \n",
    "                    if col2 not in graph:\n",
    "                        graph[col2] = []\n",
    "                    graph[col2].append(col1)\n",
    "        \n",
    "        visited = set()\n",
    "        components = []\n",
    "        \n",
    "        def dfs(node, comp):\n",
    "            visited.add(node)\n",
    "            comp.append(node)\n",
    "            if node in graph:\n",
    "                for neighbor in graph[node]:\n",
    "                    if neighbor not in visited:\n",
    "                        dfs(neighbor, comp)\n",
    "        \n",
    "        for col in self.jqfactors_list:\n",
    "            if col not in visited:\n",
    "                comp = []\n",
    "                dfs(col, comp)\n",
    "                components.append(comp)\n",
    "        \n",
    "        to_keep = []\n",
    "        to_remove = []\n",
    "        \n",
    "        for comp in components:\n",
    "            if len(comp) == 1:\n",
    "                to_keep.append(comp[0])\n",
    "            else:\n",
    "                comp_sorted = sorted(comp, key=lambda x: (missing_counts[x], x))\n",
    "                keep_feature = comp_sorted[0]\n",
    "                to_keep.append(keep_feature)\n",
    "                to_remove.extend(comp_sorted[1:])\n",
    "        \n",
    "        print(f\"Removed features: {len(to_remove)}\")\n",
    "        return to_keep\n",
    "\n",
    "    def train_models(self):\n",
    "        \"\"\"Train all models.\"\"\"\n",
    "        print(\"Training multi-model ensemble...\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv('multi_model_train_data.csv')\n",
    "            with open('multi_model_selected_features.pkl', 'rb') as f:\n",
    "                selected_features = pickle.load(f)\n",
    "            print(f\"Loaded data: {df.shape}, features: {len(selected_features)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load data: {e}\")\n",
    "            return\n",
    "        \n",
    "        X = df[selected_features]\n",
    "        y = df['label']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"Train size: {X_train.shape}, Test size: {X_test.shape}\")\n",
    "        \n",
    "        self.train_lightgbm(X_train, y_train, X_test, y_test)\n",
    "        self.train_xgboost(X_train, y_train, X_test, y_test)\n",
    "        self.train_svr(X_train, y_train, X_test, y_test)\n",
    "        self.train_random_forest(X_train, y_train, X_test, y_test)\n",
    "        self.train_linear_regression(X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        self.create_ensemble_model(X_test, y_test)\n",
    "        self.save_all_models(selected_features)\n",
    "        \n",
    "        print(\"All models trained.\")\n",
    "\n",
    "    def train_lightgbm(self, X_train, y_train, X_test, y_test):\n",
    "\n",
    "        print(\"Training LightGBM...\")\n",
    "        \n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "        \n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'binary_logloss',\n",
    "            'learning_rate': 0.05,\n",
    "            'num_leaves': 31,\n",
    "            'max_depth': -1,\n",
    "            'min_data_in_leaf': 20,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbosity': -1,\n",
    "            'random_state': 42\n",
    "        }\n",
    "        \n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            lgb_train,\n",
    "            num_boost_round=500,\n",
    "            valid_sets=[lgb_test],\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    "        )\n",
    "        \n",
    "        self.models_config['lgb']['model'] = model\n",
    "        self.evaluate_model(model, X_test, y_test, 'LightGBM')\n",
    "\n",
    "    def train_xgboost(self, X_train, y_train, X_test, y_test):\n",
    "\n",
    "        print(\"Training XGBoost...\")\n",
    "        \n",
    "        params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'learning_rate': 0.05,\n",
    "            'max_depth': 6,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'random_state': 42,\n",
    "            'n_estimators': 300\n",
    "        }\n",
    "        \n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "        \n",
    "        self.models_config['xgb']['model'] = model\n",
    "        self.evaluate_model(model, X_test, y_test, 'XGBoost')\n",
    "\n",
    "    def train_svr(self, X_train, y_train, X_test, y_test):\n",
    "\n",
    "        print(\"Training SVR...\")\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        sample_size = min(10000, len(X_train_scaled))\n",
    "        if sample_size < len(X_train_scaled):\n",
    "            indices = np.random.choice(len(X_train_scaled), sample_size, replace=False)\n",
    "            X_train_sampled = X_train_scaled[indices]\n",
    "            y_train_sampled = y_train.iloc[indices]\n",
    "        else:\n",
    "            X_train_sampled = X_train_scaled\n",
    "            y_train_sampled = y_train\n",
    "        \n",
    "        model = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "        model.fit(X_train_sampled, y_train_sampled)\n",
    "        \n",
    "        self.models_config['svr']['model'] = {'model': model, 'scaler': scaler}\n",
    "        \n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "        \n",
    "        self.print_metrics_compatible(y_test, y_pred_binary, 'SVR')\n",
    "\n",
    "    def train_random_forest(self, X_train, y_train, X_test, y_test):\n",
    "\n",
    "        print(\"Training RandomForest...\")\n",
    "        \n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=50,\n",
    "            max_depth=10,\n",
    "            min_samples_split=20,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        self.models_config['rf']['model'] = model\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "        \n",
    "        self.print_metrics_compatible(y_test, y_pred_binary, 'RandomForest')\n",
    "\n",
    "    def train_linear_regression(self, X_train, y_train, X_test, y_test):\n",
    "\n",
    "        print(\"Training LinearRegression...\")\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        self.models_config['lr']['model'] = model\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "        \n",
    "        self.print_metrics_compatible(y_test, y_pred_binary, 'LinearRegression')\n",
    "\n",
    "    def create_ensemble_model(self, X_test, y_test):\n",
    "        \"\"\"Build ensemble model.\"\"\"\n",
    "        print(\"Building ensemble...\")\n",
    "        \n",
    "        predictions = []\n",
    "        model_names = []\n",
    "        \n",
    "        if self.models_config['lgb']['model'] is not None:\n",
    "            lgb_pred = self.models_config['lgb']['model'].predict(X_test)\n",
    "            predictions.append(lgb_pred)\n",
    "            model_names.append('LightGBM')\n",
    "        \n",
    "        if self.models_config['xgb']['model'] is not None:\n",
    "            xgb_pred = self.models_config['xgb']['model'].predict_proba(X_test)[:, 1]\n",
    "            predictions.append(xgb_pred)\n",
    "            model_names.append('XGBoost')\n",
    "        \n",
    "        if self.models_config['svr']['model'] is not None:\n",
    "            svr_data = self.models_config['svr']['model']\n",
    "            scaler = svr_data['scaler']\n",
    "            svr_model = svr_data['model']\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            svr_pred = svr_model.predict(X_test_scaled)\n",
    "            svr_pred = (svr_pred - svr_pred.min()) / (svr_pred.max() - svr_pred.min() + 1e-8)\n",
    "            predictions.append(svr_pred)\n",
    "            model_names.append('SVR')\n",
    "        \n",
    "        if self.models_config['rf']['model'] is not None:\n",
    "            rf_pred = self.models_config['rf']['model'].predict(X_test)\n",
    "            rf_pred = (rf_pred - rf_pred.min()) / (rf_pred.max() - rf_pred.min() + 1e-8)\n",
    "            predictions.append(rf_pred)\n",
    "            model_names.append('RandomForest')\n",
    "        \n",
    "        if self.models_config['lr']['model'] is not None:\n",
    "            lr_pred = self.models_config['lr']['model'].predict(X_test)\n",
    "            lr_pred = (lr_pred - lr_pred.min()) / (lr_pred.max() - lr_pred.min() + 1e-8)\n",
    "            predictions.append(lr_pred)\n",
    "            model_names.append('LinearRegression')\n",
    "        \n",
    "        if len(predictions) == 0:\n",
    "            print(\"No available models for ensembling\")\n",
    "            return\n",
    "        \n",
    "        weights = [0.35, 0.35, 0.1, 0.1, 0.1]\n",
    "        weights = weights[:len(predictions)]\n",
    "        \n",
    "        ensemble_pred = np.average(predictions, axis=0, weights=weights)\n",
    "        ensemble_pred_binary = (ensemble_pred > 0.5).astype(int)\n",
    "        \n",
    "        self.print_metrics_compatible(y_test, ensemble_pred_binary, 'Ensemble')\n",
    "        \n",
    "        self.models_config['ensemble']['model'] = {\n",
    "            'models': model_names,\n",
    "            'weights': weights\n",
    "        }\n",
    "\n",
    "    def evaluate_model(self, model, X_test, y_test, model_name):\n",
    "\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        else:\n",
    "            y_pred_proba = model.predict(X_test)\n",
    "        \n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "        self.print_metrics_compatible(y_test, y_pred, model_name)\n",
    "\n",
    "    def print_metrics_compatible(self, y_true, y_pred, model_name):\n",
    "        \"\"\"Print metrics with zero_division-safe behavior.\"\"\"\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        try:\n",
    "            precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        except TypeError:\n",
    "            precision = precision_score(y_true, y_pred)\n",
    "            recall = recall_score(y_true, y_pred)\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "        \n",
    "        print(f\"{model_name} - Acc: {accuracy:.4f}, Prec: {precision:.4f}, Rec: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    def save_all_models(self, selected_features):\n",
    "\n",
    "        print(\"Saving models...\")\n",
    "        \n",
    "        model_data = {\n",
    "            'models': self.models_config,\n",
    "            'selected_features': selected_features,\n",
    "            'training_time': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            'model_type': 'multi_model_ensemble'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open('multi_model_ensemble.pkl', 'wb') as f:\n",
    "                pickle.dump(model_data, f)\n",
    "            print(\"Saved: multi_model_ensemble.pkl\")\n",
    "            \n",
    "            import base64\n",
    "            model_bytes = pickle.dumps(model_data)\n",
    "            model_base64 = base64.b64encode(model_bytes).decode('utf-8')\n",
    "            with open('multi_model_ensemble.txt', 'w') as f:\n",
    "                f.write(model_base64)\n",
    "            print(\"Backup saved: multi_model_ensemble.txt\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save models: {e}\")\n",
    "\n",
    "# Simple test entry\n",
    "def test_multi_model():\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Start testing multi-model ensemble\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    research = MultiModelResearch()\n",
    "    \n",
    "    print(\"Step 1: Generate training data...\")\n",
    "    data, features = research.generate_training_data()\n",
    "    \n",
    "    print(\"Step 2: Train models...\")\n",
    "    research.train_models()\n",
    "    \n",
    "    print(\"Test finished.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_multi_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6eb658-3d34-43aa-b47f-a7fe76a31151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jqdata import *\n",
    "from jqfactor import get_factor_values\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def initialize(context):\n",
    "    # Benchmark\n",
    "    set_benchmark('399101.XSHE')\n",
    "    # Use real price\n",
    "    set_option('use_real_price', True)\n",
    "    # Avoid future data\n",
    "    set_option(\"avoid_future_data\", True)\n",
    "    # Zero slippage\n",
    "    set_slippage(FixedSlippage(0))\n",
    "    # Trading cost\n",
    "    set_order_cost(\n",
    "        OrderCost(open_tax=0, close_tax=0.001, open_commission=0.0003, close_commission=0.0003,\n",
    "                  close_today_commission=0, min_commission=5),\n",
    "        type='stock'\n",
    "    )\n",
    "    # Suppress order logs below error level\n",
    "    log.set_level('order', 'error')\n",
    "\n",
    "    # Globals\n",
    "    g.stock_num = 10\n",
    "    g.hold_list = []\n",
    "    g.yesterday_HL_list = []\n",
    "    g.pass_months = True\n",
    "    \n",
    "    # Model choice\n",
    "    g.model_type = 'ensemble'\n",
    "    \n",
    "    # Raw factor list\n",
    "    g.factor_list = [\n",
    "        'asset_impairment_loss_ttm', 'cash_flow_to_price_ratio', 'market_cap', \n",
    "        'interest_free_current_liability', 'EBITDA', 'financial_assets', \n",
    "        'gross_profit_ttm', 'net_working_capital', 'non_recurring_gain_loss', 'EBIT',\n",
    "        'sales_to_price_ratio', 'AR', 'ARBR', 'ATR6', 'DAVOL10', 'MAWVAD', 'TVMA6', \n",
    "        'PSY', 'VOL10', 'VDIFF', 'VEMA26', 'VMACD', 'VOL120', 'VOSC', 'VR', 'WVAD', \n",
    "        'arron_down_25', 'arron_up_25', 'BBIC', 'MASS', 'Rank1M', 'single_day_VPT', \n",
    "        'single_day_VPT_12', 'single_day_VPT_6', 'Volume1M',\n",
    "        'capital_reserve_fund_per_share', 'net_asset_per_share', \n",
    "        'net_operate_cash_flow_per_share', 'operating_profit_per_share', \n",
    "        'total_operating_revenue_per_share', 'surplus_reserve_fund_per_share',\n",
    "        'ACCA', 'account_receivable_turnover_days', 'account_receivable_turnover_rate', \n",
    "        'adjusted_profit_to_total_profit', 'super_quick_ratio', 'MLEV', \n",
    "        'debt_to_equity_ratio', 'debt_to_tangible_equity_ratio', \n",
    "        'equity_to_fixed_asset_ratio', 'fixed_asset_ratio', 'intangible_asset_ratio', \n",
    "        'invest_income_associates_to_total_profit', 'long_debt_to_asset_ratio',\n",
    "        'long_debt_to_working_capital_ratio', 'net_operate_cash_flow_to_total_liability', \n",
    "        'net_operating_cash_flow_coverage', 'non_current_asset_ratio', \n",
    "        'operating_profit_to_total_profit', 'roa_ttm', 'roe_ttm', 'Kurtosis120', \n",
    "        'Kurtosis20', 'Kurtosis60', 'sharpe_ratio_20', 'sharpe_ratio_60', \n",
    "        'Skewness120', 'Skewness20', 'Skewness60', 'Variance120', 'Variance20', \n",
    "        'liquidity', 'beta', 'book_to_price_ratio', 'cash_earnings_to_price_ratio', \n",
    "        'cube_of_size', 'earnings_to_price_ratio', 'earnings_yield', 'growth', \n",
    "        'momentum', 'natural_log_of_market_cap', 'boll_down', 'MFI14', 'MAC10', \n",
    "        'fifty_two_week_close_rank', 'price_no_fq'\n",
    "    ]\n",
    "    \n",
    "    # Load ensemble\n",
    "    try:\n",
    "        model_data = pickle.loads(read_file('multi_model_ensemble.pkl'))\n",
    "        g.models_config = model_data['models']\n",
    "        g.model_features = model_data['selected_features']\n",
    "        \n",
    "        log.info(\"===== Ensemble loaded successfully =====\")\n",
    "        log.info(\"Models available: {}\".format([g.models_config[key]['name'] for key in g.models_config if g.models_config[key]['model'] is not None]))\n",
    "        log.info(\"Num features: {}\".format(len(g.model_features)))\n",
    "        log.info(\"Using model: {}\".format(g.model_type))\n",
    "        \n",
    "    except Exception as e:\n",
    "        log.error(\"Failed to load model: {}\".format(e))\n",
    "        try:\n",
    "            import base64\n",
    "            model_base64 = read_file('multi_model_ensemble.txt')\n",
    "            model_bytes = base64.b64decode(model_base64)\n",
    "            model_data = pickle.loads(model_bytes)\n",
    "            g.models_config = model_data['models']\n",
    "            g.model_features = model_data['selected_features']\n",
    "            log.info(\"Loaded model from Base64 backup\")\n",
    "        except Exception as e2:\n",
    "            log.error(\"Backup load failed: {}\".format(e2))\n",
    "            g.models_config = None\n",
    "            g.model_features = g.factor_list\n",
    "\n",
    "    run_daily(prepare_stock_list, '9:05')\n",
    "    run_monthly(weekly_adjustment, 1, '9:50')\n",
    "    run_monthly(weekly_adjustment, 15, '9:50')\n",
    "    run_daily(check_limit_up, '10:30')\n",
    "    run_daily(check_limit_up, '14:00')\n",
    "\n",
    "def prepare_stock_list(context):\n",
    "    \"\"\"Prepare current holdings and yesterday's limit-up list.\"\"\"\n",
    "    g.hold_list = []\n",
    "    for position in list(context.portfolio.positions.values()):\n",
    "        stock = position.security\n",
    "        g.hold_list.append(stock)\n",
    "\n",
    "    if g.hold_list:\n",
    "        df = get_price(g.hold_list, end_date=context.previous_date, frequency='daily',\n",
    "                       fields=['close', 'high_limit'], count=1, panel=False, fill_paused=False)\n",
    "        df = df[df['close'] == df['high_limit']]\n",
    "        g.yesterday_HL_list = list(df.code) if len(df) > 0 else []\n",
    "    else:\n",
    "        g.yesterday_HL_list = []\n",
    "\n",
    "def get_stock_list(context):\n",
    "\n",
    "    yesterday = context.previous_date\n",
    "    stocks = get_index_stocks('399101.XSHE', yesterday)\n",
    "\n",
    "    initial_list = filter_kcbj_stock(stocks)         \n",
    "    initial_list = filter_st_stock(initial_list)     \n",
    "    initial_list = filter_paused_stock(initial_list) \n",
    "    initial_list = filter_new_stock(context, initial_list)\n",
    "    initial_list = filter_limitup_stock(context, initial_list)\n",
    "    initial_list = filter_limitdown_stock(context, initial_list)\n",
    "\n",
    "    if g.model_features is None or len(g.model_features) == 0:\n",
    "        log.error(\"Empty feature list, cannot predict\")\n",
    "        return []\n",
    "\n",
    "    if not initial_list:\n",
    "        log.info(\"No usable stocks after filters\")\n",
    "        return []\n",
    "\n",
    "    req_factors = [f for f in g.model_features if f in g.factor_list]\n",
    "    \n",
    "    if not req_factors:\n",
    "        log.error(\"No usable factor data\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        factor_data = get_factor_values(initial_list, req_factors, end_date=yesterday, count=1)\n",
    "    except Exception as e:\n",
    "        log.error(\"Failed to fetch factor data: {}\".format(e))\n",
    "        return []\n",
    "\n",
    "    df_jq_factor_value = pd.DataFrame(index=initial_list)\n",
    "    for f in req_factors:\n",
    "        df_jq_factor_value[f] = list(factor_data[f].T.iloc[:, 0])\n",
    "\n",
    "    df_jq_factor_value = df_jq_factor_value.reindex(columns=g.model_features)\n",
    "    df_jq_factor_value = df_jq_factor_value.astype(np.float64)\n",
    "    df_jq_factor_value = df_jq_factor_value.fillna(0)\n",
    "\n",
    "    try:\n",
    "        if g.model_type == 'lightgbm':\n",
    "            model = g.models_config['lgb']['model']\n",
    "            tar = model.predict(df_jq_factor_value)\n",
    "            \n",
    "        elif g.model_type == 'xgboost':\n",
    "            model = g.models_config['xgb']['model']\n",
    "            tar = model.predict_proba(df_jq_factor_value)[:, 1]\n",
    "            \n",
    "        elif g.model_type == 'svr':\n",
    "            svr_data = g.models_config['svr']['model']\n",
    "            scaler = svr_data['scaler']\n",
    "            model = svr_data['model']\n",
    "            X_scaled = scaler.transform(df_jq_factor_value)\n",
    "            tar = model.predict(X_scaled)\n",
    "            tar = (tar - tar.min()) / (tar.max() - tar.min())\n",
    "            \n",
    "        elif g.model_type == 'random_forest':\n",
    "            model = g.models_config['rf']['model']\n",
    "            tar = model.predict(df_jq_factor_value)\n",
    "            tar = (tar - tar.min()) / (tar.max() - tar.min())\n",
    "            \n",
    "        elif g.model_type == 'linear':\n",
    "            model = g.models_config['lr']['model']\n",
    "            tar = model.predict(df_jq_factor_value)\n",
    "            tar = (tar - tar.min()) / (tar.max() - tar.min())\n",
    "            \n",
    "        elif g.model_type == 'ensemble':\n",
    "            predictions = []\n",
    "            weights = g.models_config['ensemble']['model']['weights']\n",
    "            \n",
    "            lgb_pred = g.models_config['lgb']['model'].predict(df_jq_factor_value)\n",
    "            predictions.append(lgb_pred)\n",
    "            \n",
    "            xgb_pred = g.models_config['xgb']['model'].predict_proba(df_jq_factor_value)[:, 1]\n",
    "            predictions.append(xgb_pred)\n",
    "            \n",
    "            svr_data = g.models_config['svr']['model']\n",
    "            scaler = svr_data['scaler']\n",
    "            svr_model = svr_data['model']\n",
    "            X_scaled = scaler.transform(df_jq_factor_value)\n",
    "            svr_pred = svr_model.predict(X_scaled)\n",
    "            svr_pred = (svr_pred - svr_pred.min()) / (svr_pred.max() - svr_pred.min())\n",
    "            predictions.append(svr_pred)\n",
    "            \n",
    "            rf_pred = g.models_config['rf']['model'].predict(df_jq_factor_value)\n",
    "            rf_pred = (rf_pred - rf_pred.min()) / (rf_pred.max() - rf_pred.min())\n",
    "            predictions.append(rf_pred)\n",
    "            \n",
    "            lr_pred = g.models_config['lr']['model'].predict(df_jq_factor_value)\n",
    "            lr_pred = (lr_pred - lr_pred.min()) / (lr_pred.max() - lr_pred.min())\n",
    "            predictions.append(lr_pred)\n",
    "            \n",
    "            tar = np.average(predictions, axis=0, weights=weights)\n",
    "            \n",
    "        else:\n",
    "            log.error(\"Unknown model type: {}\".format(g.model_type))\n",
    "            return []\n",
    "            \n",
    "    except Exception as e:\n",
    "        log.error(\"Model inference failed: {}\".format(e))\n",
    "        import traceback\n",
    "        log.error(\"Traceback: {}\".format(traceback.format_exc()))\n",
    "        return []\n",
    "\n",
    "    df = df_jq_factor_value.copy()\n",
    "    df['total_score'] = list(tar)\n",
    "    df = df.sort_values(by=['total_score'], ascending=False)\n",
    "    lst = df.index.tolist()\n",
    "    lst = lst[:min(g.stock_num, len(lst))]\n",
    "    \n",
    "    if len(lst) > 0:\n",
    "        log.info(\"=== {} selection Top {} ===\".format(g.model_type, len(lst)))\n",
    "        for i, stock in enumerate(lst, 1):\n",
    "            score = df.loc[stock, 'total_score']\n",
    "            stock_name = get_security_info(stock).display_name\n",
    "            log.info(\"{}. {} {} - score: {:.4f}\".format(i, stock, stock_name, score))\n",
    "    \n",
    "    return lst\n",
    "\n",
    "def filter_paused_stock(stock_list):\n",
    "\n",
    "    current_data = get_current_data()\n",
    "    return [stock for stock in stock_list if not current_data[stock].paused]\n",
    "\n",
    "def filter_st_stock(stock_list):\n",
    "    \"\"\"Filter ST/*ST/delisted-like names.\"\"\"\n",
    "    current_data = get_current_data()\n",
    "    return [stock for stock in stock_list\n",
    "            if not current_data[stock].is_st\n",
    "            and 'ST' not in current_data[stock].name\n",
    "            and '*' not in current_data[stock].name\n",
    "            and 'é€€' not in current_data[stock].name]\n",
    "\n",
    "def filter_kcbj_stock(stock_list):\n",
    "    \"\"\"Filter special boards not suitable for trading.\"\"\"\n",
    "    res = []\n",
    "    for stock in stock_list:\n",
    "        if (stock[:2] == '68') or (stock[0] in ['4', '8']) or (stock[0] == '3'):\n",
    "            continue\n",
    "        res.append(stock)\n",
    "    return res\n",
    "\n",
    "def filter_limitup_stock(context, stock_list):\n",
    "\n",
    "    if not stock_list:\n",
    "        return []\n",
    "    last_prices = history(1, unit='1m', field='close', security_list=stock_list)\n",
    "    current_data = get_current_data()\n",
    "    res = []\n",
    "    for stock in stock_list:\n",
    "        in_pos = (stock in context.portfolio.positions.keys())\n",
    "        if in_pos:\n",
    "            res.append(stock)\n",
    "        else:\n",
    "            try:\n",
    "                if last_prices[stock][-1] < current_data[stock].high_limit:\n",
    "                    res.append(stock)\n",
    "            except Exception:\n",
    "                res.append(stock)\n",
    "    return res\n",
    "\n",
    "def filter_limitdown_stock(context, stock_list):\n",
    "    \"\"\"Filter limit-down stocks (except current holdings).\"\"\"\n",
    "    if not stock_list:\n",
    "        return []\n",
    "    last_prices = history(1, unit='1m', field='close', security_list=stock_list)\n",
    "    current_data = get_current_data()\n",
    "    res = []\n",
    "    for stock in stock_list:\n",
    "        in_pos = (stock in context.portfolio.positions.keys())\n",
    "        if in_pos:\n",
    "            res.append(stock)\n",
    "        else:\n",
    "            try:\n",
    "                if last_prices[stock][-1] > current_data[stock].low_limit:\n",
    "                    res.append(stock)\n",
    "            except Exception:\n",
    "                res.append(stock)\n",
    "    return res\n",
    "\n",
    "def filter_new_stock(context, stock_list):\n",
    "\n",
    "    yesterday = context.previous_date\n",
    "    return [stock for stock in stock_list\n",
    "            if not (yesterday - get_security_info(stock).start_date < datetime.timedelta(days=375))]\n",
    "\n",
    "def weekly_adjustment(context):\n",
    "\n",
    "    target_list = get_stock_list(context)\n",
    "\n",
    "    for stock in g.hold_list:\n",
    "        if (stock not in target_list) and (stock not in g.yesterday_HL_list):\n",
    "            log.info(\"Sell [%s]\" % (stock))\n",
    "            position = context.portfolio.positions.get(stock, None)\n",
    "            if position:\n",
    "                close_position(position)\n",
    "        else:\n",
    "            log.info(\"Keep [%s]\" % (stock))\n",
    "\n",
    "    position_count = len(context.portfolio.positions)\n",
    "    target_num = len(target_list)\n",
    "    \n",
    "    if target_num > position_count:\n",
    "        per_value = context.portfolio.cash / float(target_num - position_count) if (target_num - position_count) > 0 else 0.0\n",
    "        for stock in target_list:\n",
    "            pos = context.portfolio.positions.get(stock, None)\n",
    "            already_holding = (pos is not None and pos.total_amount > 0)\n",
    "            if not already_holding:\n",
    "                if per_value > 0 and open_position(stock, per_value):\n",
    "                    if len(context.portfolio.positions) >= target_num:\n",
    "                        break\n",
    "\n",
    "def check_limit_up(context):\n",
    "    \"\"\"Check if yesterday's limit-up opens today.\"\"\"\n",
    "    now_time = context.current_dt\n",
    "    if g.yesterday_HL_list:\n",
    "        for stock in g.yesterday_HL_list:\n",
    "            current_data = get_price(stock, end_date=now_time, frequency='1m',\n",
    "                                     fields=['close', 'high_limit'],\n",
    "                                     skip_paused=False, fq='pre', count=1,\n",
    "                                     panel=False, fill_paused=True)\n",
    "            if len(current_data) > 0:\n",
    "                close_px = current_data.iloc[0]['close']\n",
    "                high_lim = current_data.iloc[0]['high_limit']\n",
    "                if close_px < high_lim:\n",
    "                    log.info(\"[%s] limit-up opened, sell\" % (stock))\n",
    "                    position = context.portfolio.positions.get(stock, None)\n",
    "                    if position:\n",
    "                        close_position(position)\n",
    "                else:\n",
    "                    log.info(\"[%s] still limit-up, hold\" % (stock))\n",
    "\n",
    "def order_target_value_(security, value):\n",
    "\n",
    "    if value == 0:\n",
    "        log.debug(\"Close position %s\" % (security))\n",
    "    else:\n",
    "        log.debug(\"Order %s target value %f\" % (security, value))\n",
    "    return order_target_value(security, value)\n",
    "\n",
    "def open_position(security, value):\n",
    "\n",
    "    order_obj = order_target_value_(security, value)\n",
    "    if order_obj is not None and order_obj.filled > 0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def close_position(position):\n",
    "\n",
    "    security = position.security\n",
    "    order_obj = order_target_value_(security, 0)\n",
    "    if order_obj is not None:\n",
    "        if (order_obj.status == OrderStatus.held) and (order_obj.filled == order_obj.amount):\n",
    "            return True\n",
    "    return False\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
